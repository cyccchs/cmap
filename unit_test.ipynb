{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_glimpse(img_batch, size, offsets, centered=True, normalized=True):\n",
    "    W, H = img_batch.size(-1), img_batch.size(-2)\n",
    "\n",
    "    if normalized and centered:\n",
    "        offsets = (offsets+1) * offsets.new_tensor([W/2,H/2])\n",
    "    elif normalized:\n",
    "        offsets = offsets * offsets.new_tensor([W,H])\n",
    "    elif centered:\n",
    "        raise ValueError(f'Invalid parameter that offsets centered but now normalized')\n",
    "\n",
    "    h, w = size\n",
    "    xs = torch.arange(0, w, dtype=img_batch.dtype, device=img_batch.device) - (w-1)/2.0\n",
    "    ys = torch.arange(0, h, dtype=img_batch.dtype, device=img_batch.device) - (h-1)/2.0\n",
    "\n",
    "    vy, vx = torch.meshgrid(ys, xs)\n",
    "    grid = torch.stack([vx, vy], dim=-1)\n",
    "\n",
    "    offsets_grid = offsets[:,None, None, :] + grid[None, ...]\n",
    "    offsets_grid = (\n",
    "            offsets_grid - offsets_grid.new_tensor([W/2,H/2])) / offsets_grid.new_tensor([W/2,H/2])\n",
    "\n",
    "    return torch.nn.functional.grid_sample(\n",
    "            img_batch, offsets_grid, mode='bilinear', align_corners=False, padding_mode='zeros')\n",
    "\n",
    "def extract_multiple_glimpse(img_batch, size, offsets, centered=True, normalized=True):\n",
    "    patches = []\n",
    "\n",
    "    for i in range(offsets.size(-2)):\n",
    "        patch = extract_glimpse(\n",
    "                img_batch, size, offsets[:, i, :], centered, normalized)\n",
    "        patches.append(patch)\n",
    "\n",
    "    return torch.stack(patches, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = torch.zeros([8,3,64,64], dtype=torch.float32)\n",
    "glimpse_size = tuple([3,3])\n",
    "offsets = torch.ones([8,2],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.zeros([8,2], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1da06e0f9ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "torch.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dab3b841641f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_multiple_glimpse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglimpse_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1eaac86bcb32>\u001b[0m in \u001b[0;36mextract_multiple_glimpse\u001b[0;34m(img_batch, size, offsets, centered, normalized)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         patch = extract_glimpse(\n\u001b[0;32m---> 33\u001b[0;31m                 img_batch, size, offsets[:, i, :], centered, normalized)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "extract_multiple_glimpse(img_batch, glimpse_size, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
